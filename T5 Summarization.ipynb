{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfromer T5 Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu117/torch-2.0.0%2Bcu117-cp39-cp39-win_amd64.whl (2343.7 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu117/torchvision-0.15.1%2Bcu117-cp39-cp39-win_amd64.whl (4.9 MB)\n",
      "Requirement already satisfied: torchaudio in d:\\anaconda64bit\\lib\\site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: networkx in c:\\users\\55467\\appdata\\roaming\\python\\python39\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\55467\\appdata\\roaming\\python\\python39\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\55467\\appdata\\roaming\\python\\python39\\site-packages (from torch) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda64bit\\lib\\site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\55467\\appdata\\roaming\\python\\python39\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\55467\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda64bit\\lib\\site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda64bit\\lib\\site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\55467\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda64bit\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda64bit\\lib\\site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda64bit\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda64bit\\lib\\site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\55467\\appdata\\roaming\\python\\python39\\site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-2.0.0+cu117 torchvision-0.15.1+cu117\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers \n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "# Code for TPU packages install, only applied when using cloud\n",
    "#!curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "#!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nivida-smi' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!nivida-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
    "#https://zhuanlan.zhihu.com/p/424565138 batch_tockenizer\n",
    "# squeeze https://edu.csdn.net/courses/o280_s355_z1/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E6%A1%86%E6%9E%B6_Python?utm_source=order0\n",
    "class CustomDataset():\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.text\n",
    "        self.ctext = self.data.ctext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "    #questions: why convert it to long type? why squeeze?\n",
    "\n",
    "    # Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
    "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n",
    "\n",
    "    def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "        model.train()\n",
    "        #https://www.datacamp.com/tutorial/role-underscore-python what does _ means\n",
    "        #https://www.geeksforgeeks.org/enumerate-in-python/ what emumerate does\n",
    "        for _,data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            y_ids = y[:, :-1].contiguous()\n",
    "            lm_labels = y[:, 1:].clone().detach()\n",
    "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            if _%500==0:\n",
    "                print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # xm.optimizer_step(optimizer)\n",
    "            # xm.mark_step()\n",
    "\n",
    "    def validate(epoch, tokenizer, model, device, loader):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(loader, 0):\n",
    "                y = data['target_ids'].to(device, dtype = torch.long)\n",
    "                ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "                mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids = ids,\n",
    "                    attention_mask = mask, \n",
    "                    max_length=150, \n",
    "                    num_beams=2,\n",
    "                    repetition_penalty=2.5, \n",
    "                    length_penalty=1.0, \n",
    "                    early_stopping=True\n",
    "                    )\n",
    "                preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "                target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "                if _%100==0:\n",
    "                    print(f'Completed {_}')\n",
    "\n",
    "                predictions.extend(preds)\n",
    "                actuals.extend(target)\n",
    "        return predictions, actuals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOutput Files generated for review\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 89\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m SUMMARY_LEN \u001b[39m=\u001b[39m \u001b[39m150\u001b[39m \n\u001b[0;32m     12\u001b[0m \u001b[39m# Set random seeds and deterministic pytorch for reproducibility\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(SEED) \u001b[39m# pytorch random seed\u001b[39;00m\n\u001b[0;32m     14\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(SEED) \u001b[39m# numpy random seed\u001b[39;00m\n\u001b[0;32m     15\u001b[0m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39mdeterministic \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "    VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "    TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
    "    VAL_EPOCHS = 1 \n",
    "    LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "    SEED = 42               # random seed (default: 42)\n",
    "    MAX_LEN = 512\n",
    "    SUMMARY_LEN = 150 \n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(SEED) # pytorch random seed\n",
    "    np.random.seed(SEED) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    \n",
    "\n",
    "    # Importing and Pre-Processing the domain data\n",
    "    # Selecting the needed columns only. \n",
    "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "    df = pd.read_csv('../input/news-summary/news_summary.csv',encoding='latin-1')\n",
    "    df = df[['text','ctext']]\n",
    "    df.ctext = 'summarize: ' + df.ctext\n",
    "    print(df.head())\n",
    "\n",
    "    \n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\n",
    "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "    val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        'batch_size': TRAIN_BATCH_SIZE,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': VALID_BATCH_SIZE,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    \n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "    for epoch in range(TRAIN_EPOCHS):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "    # Saving the dataframe as predictions.csv\n",
    "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "    for epoch in range(VAL_EPOCHS):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv('predictions.csv')\n",
    "        print('Output Files generated for review')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
